{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d158650-3806-427e-a7dc-ddd18a03e7f3",
   "metadata": {},
   "source": [
    "## IID (Independent and Identically Distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b5cc2a3-5d81-45f8-ad1e-05453179a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80912a49-4768-4f45-96da-536953546a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3612435-b2d1-4803-b3d0-2bdb33b4c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 class distribution: {0: 81, 1: 104, 2: 95, 3: 80, 4: 88, 5: 82, 6: 98, 7: 105, 8: 80, 9: 96} (Total: 909)\n",
      "Client 1 class distribution: {0: 150, 1: 175, 2: 138, 3: 135, 4: 132, 5: 120, 6: 148, 7: 166, 8: 133, 9: 150} (Total: 1447)\n",
      "Client 2 class distribution: {0: 39, 1: 36, 2: 45, 3: 33, 4: 54, 5: 36, 6: 35, 7: 30, 8: 45, 9: 40} (Total: 393)\n",
      "Client 3 class distribution: {0: 61, 1: 84, 2: 80, 3: 77, 4: 81, 5: 81, 6: 82, 7: 68, 8: 70, 9: 73} (Total: 757)\n",
      "Client 4 class distribution: {0: 54, 1: 79, 2: 62, 3: 47, 4: 49, 5: 59, 6: 53, 7: 55, 8: 56, 9: 56} (Total: 570)\n",
      "Client 5 class distribution: {0: 12, 1: 26, 2: 33, 3: 29, 4: 17, 5: 22, 6: 17, 7: 23, 8: 22, 9: 15} (Total: 216)\n",
      "Client 6 class distribution: {1: 2, 2: 2, 3: 1, 4: 1, 5: 1, 8: 2} (Total: 9)\n",
      "Client 7 class distribution: {0: 13, 1: 25, 2: 28, 3: 18, 4: 24, 5: 22, 6: 16, 7: 19, 8: 17, 9: 18} (Total: 200)\n",
      "Client 8 class distribution: {0: 21, 1: 24, 2: 31, 3: 30, 4: 23, 5: 16, 6: 25, 7: 28, 8: 24, 9: 22} (Total: 244)\n",
      "Client 9 class distribution: {0: 115, 1: 154, 2: 121, 3: 121, 4: 127, 5: 136, 6: 123, 7: 109, 8: 122, 9: 127} (Total: 1255)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Load MNIST training data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Parameters\n",
    "num_clients = 10\n",
    "total_samples = 6000\n",
    "\n",
    "# Extract labels\n",
    "labels = np.array(mnist_train.targets)\n",
    "\n",
    "# Step 1: Sample 6000 indices from the entire dataset to preserve global distribution\n",
    "all_indices = np.arange(len(labels))\n",
    "np.random.shuffle(all_indices)\n",
    "\n",
    "# Choose 6000 samples\n",
    "selected_indices = all_indices[:total_samples]\n",
    "selected_labels = labels[selected_indices]\n",
    "\n",
    "# Step 2: Randomly assign samples to clients (with varying sizes)\n",
    "client_indices = defaultdict(list)\n",
    "\n",
    "# Generate random proportions for each client (but sum to 1)\n",
    "proportions = np.random.dirichlet(np.ones(num_clients), size=1)[0]\n",
    "client_sizes = (proportions * total_samples).astype(int)\n",
    "\n",
    "# Fix rounding issue to ensure exactly 6000 samples\n",
    "diff = total_samples - np.sum(client_sizes)\n",
    "client_sizes[np.argmax(client_sizes)] += diff\n",
    "\n",
    "# Now split selected_indices into those sizes\n",
    "start = 0\n",
    "for client_id, size in enumerate(client_sizes):\n",
    "    client_indices[client_id] = selected_indices[start:start + size].tolist()\n",
    "    start += size\n",
    "\n",
    "# Create Subset datasets for each client\n",
    "client_datasets = [Subset(mnist_train, client_indices[i]) for i in range(num_clients)]\n",
    "\n",
    "# Debug: show class distribution per client\n",
    "for i, dataset in enumerate(client_datasets):\n",
    "    labels = [mnist_train.targets[idx].item() for idx in dataset.indices]\n",
    "    label_count = dict(sorted(Counter(labels).items()))\n",
    "    print(f\"Client {i} class distribution: {label_count} (Total: {len(dataset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5cfa16-4fcf-441f-8510-8c24568bac0c",
   "metadata": {},
   "source": [
    "## Non-IID (Independent and Identically Distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150c4a38-b482-4007-abda-e250f24251f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noniid_shards(dataset, num_clients=10, shards_per_client=2):\n",
    "    labels = np.array(dataset.targets)\n",
    "    data_indices = np.arange(len(labels))\n",
    "\n",
    "    # Sort by label\n",
    "    sorted_indices = data_indices[np.argsort(labels)]\n",
    "\n",
    "    # Create 200 shards of size 300\n",
    "    num_shards = num_clients * shards_per_client\n",
    "    shard_size = len(dataset) // num_shards\n",
    "    shards = [sorted_indices[i * shard_size:(i + 1) * shard_size] for i in range(num_shards)]\n",
    "\n",
    "    # Shuffle and assign shards to clients\n",
    "    np.random.shuffle(shards)\n",
    "    client_indices = [[] for _ in range(num_clients)]\n",
    "    \n",
    "    for client_id in range(num_clients):\n",
    "        for i in range(shards_per_client):\n",
    "            shard = shards[client_id * shards_per_client + i]\n",
    "            client_indices[client_id].extend(shard)\n",
    "\n",
    "    # Create Subset datasets for each client\n",
    "    client_datasets = [Subset(dataset, indices) for indices in client_indices]\n",
    "    return client_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908c2cf1-3c7d-4833-9b9e-0b26619f5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "client_datasets = create_noniid_shards(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88bc8b01-4cfb-4f0e-a134-1ee0cbc81b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 label distribution: {0: 2923, 1: 742, 2: 2335}\n",
      "Client 1 label distribution: {2: 623, 3: 2377, 7: 3000}\n",
      "Client 2 label distribution: {1: 3000, 3: 3000}\n",
      "Client 3 label distribution: {5: 3017, 6: 2983}\n",
      "Client 4 label distribution: {7: 3000, 2: 3000}\n",
      "Client 5 label distribution: {4: 3000, 9: 3000}\n",
      "Client 6 label distribution: {7: 200, 8: 2851, 9: 2949}\n",
      "Client 7 label distribution: {0: 3000, 6: 2935, 7: 65}\n",
      "Client 8 label distribution: {4: 2842, 5: 2404, 3: 754}\n",
      "Client 9 label distribution: {8: 3000, 1: 3000}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for i, client_data in enumerate(client_datasets):\n",
    "    labels = [mnist_train.targets[idx].item() for idx in client_data.indices]\n",
    "    dist = Counter(labels)\n",
    "    print(f\"Client {i} label distribution: {dict(dist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b61e32-09aa-4ed0-9b5c-a1c5a0f33904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
